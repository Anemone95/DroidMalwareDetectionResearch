#!/usr/bin/env python
# coding=utf-8

# @file crawler.py
# @brief crawler
# @author x565178035,x565178035@126.com
# @version 1.0
# @date 2017-12-10 09:41
import requests
import collections
import time
import random
import cPickle as pickle
from lxml import etree

Paper = collections.namedtuple(
    'Paper', ['title', 'url', 'year', 'include'])

google_url = "https://scholar.google.com"
google_url = "https://xue.glgoo.net"
#  google_url = "https://g.kingyou.wang"

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36',
    'Referer': google_url}

proxies = {
    "http": "http://127.0.0.1:8080",
    "https": "http://127.0.0.1:8080"}
proxies = None


def force_get(session, url, params=None):
    while True:
        try:
            time.sleep(random.uniform(0,5))
            res = session.get(
                url,
                params=params,
                verify=False,
                headers=headers,
                proxies=proxies)
            res.encoding='utf-8'
            break
        except Exception as e:
            print e
            pass

    return res


def get_year_publish(mix_arr):
    year, include = mix_arr[-1].replace(" ", "").split("-")[:2]

    return year, include
def get_one_paper(selector):
    title = selector.xpath(
        '//h3[contains(@class,"gs_rt")]/a/text()')[0]
    url = selector.xpath(
        '//div[contains(@class,"gs_or_ggsm")]/a/@href')
    if len(url) > 0:
        url = url[0]
    else:
        url = selector.xpath(
            '//h3[contains(@class,"gs_rt")]/a/@href')[0]
    year_publish = selector\
        .xpath('//div[contains(@class,"gs_a")]')[0]\
        .xpath("string(.)")\
        .encode('utf8')\
        .split(",")
    year, include = get_year_publish(
        year_publish)
    paper = Paper(
        title=title,
        url=url,
        year=year,
        include=include)
    return paper

def crawl_one_page(session, url, params=None):
    res = force_get(session, url, params=params)
    xml_selector = etree.HTML(res.text)
    paper_divs = xml_selector.xpath('//div[contains(@class,"gs_scl")]')

    papers=[]
    for each in paper_divs:
        each_str = etree.tostring(each)
        each_selector = etree.fromstring(each_str)
        #  paper=get_one_paper(each_selector)
        #  papers.append(paper)
        try:
            paper=get_one_paper(each_selector)
            papers.append(paper)
        except Exception,e:
            print e
            print url
            print each_str

    next_url=xml_selector.xpath('//span[contains(@class,"gs_ico_nav_next")]/../@href')
    if len(next_url)>0:
        next_url=next_url[0]
    else:
        next_url=None
    return papers,next_url


def paper_based_crawler(paper_name):
    request_session = requests.session()

    # XXX:https://xue.glgoo.net会需要cookies
    #  cookies={
    #  "xid":"233b4de6d7819e307049acadc669f467", #
    #  }
    #  request_session.cookies=requests.utils.cookiejar_from_dict(cookies, cookiejar=None, overwrite=True)
    #  force_get(request_session,google_url)
    #  print request_session.cookies

    """获取该文献的引用"""
    params = {
        "hl": "en",
        "as_sdt": "0,5",
        "q": paper_name,
        "btnG": ""}
    url = google_url + "/scholar"

    res = force_get(request_session, url, params)
    xml_selector = etree.HTML(res.text)
    cites_url = xml_selector.xpath(
        '//div[contains(@class,"gs_fl")]/a[3]/@href')[0]

    """获取所有引用的文献"""
    papers,next_url=crawl_one_page(request_session,google_url+cites_url,params={"as_ylo": 2016})
    while next_url!=None:
        one_page_papers,next_url=crawl_one_page(request_session,google_url+next_url)
        papers+=one_page_papers
    with open('./{0}.csv'.format(paper_name.replace(" ","").replace(":","_")), 'w') as f:
        for each in papers:
            try:
                f.write('{title},{url},{year},{include}\n'.format(
                    title=each.title,
                    url=each.url,
                    year=each.year,
                    include=each.include,
                    ))
            except Exception,e:
                pass


if __name__ == '__main__':
    paper_based_crawler("Dissecting android malware: Characterization and evolution")
